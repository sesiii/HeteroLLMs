{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fcabddb-1e8f-425c-9bf0-ee26f06e6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cf618-8fb5-44b0-9b41-2f5af8dc69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    step: str\n",
    "    models: list[str]\n",
    "    domains: list[str]\n",
    "    subdomains: dict[str, list[str]]\n",
    "    query: str\n",
    "    intent: str\n",
    "    metrics: dict\n",
    "    context: str\n",
    "    selected_llm: str\n",
    "\n",
    "memory = MemorySaver()\n",
    "client = MultiServerMCPClient({\n",
    "    \"csv_r\": {\n",
    "        \"command\": \"python3\",\n",
    "        \"args\": [\n",
    "            \"-u\",\n",
    "            \"/home/sesi/testing/agents/ER/agentic-ai/experiments/HeteroLLMs/mcp_server/main.py\"\n",
    "        ],\n",
    "        \"cwd\": \"/home/sesi/testing/agents/ER/agentic-ai/experiments/HeteroLLMs/mcp_server/\",\n",
    "        \"transport\": \"stdio\",\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa504efe-8199-483d-a850-dfa20537aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system=system\n",
    "        self.model = model.bind_tools(tools)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.graph = self._build_graph(checkpointer)\n",
    "        self.metric_weights = {\n",
    "            \"accuracy\": 0.7,\n",
    "            \"latency_ms\": 0.2,\n",
    "            \"memory_mb\": 0.1,\n",
    "        }\n",
    "\n",
    "    def _build_graph(self, cp):\n",
    "        g = StateGraph(AgentState)\n",
    "        g.add_node(\"init\", self.init_step)\n",
    "        g.add_node(\"collect_metrics\", self.collect_metrics_step)\n",
    "        g.add_node(\"respond\", self.respond_step)\n",
    "        g.add_edge(\"init\", \"collect_metrics\")\n",
    "        g.add_edge(\"collect_metrics\", \"respond\")\n",
    "        g.set_entry_point(\"init\")\n",
    "        return g.compile(checkpointer=cp)\n",
    "\n",
    "    def classify_intent(self, query) -> str:\n",
    "        print(\"DEBUG: helper function 'classify_intent'\")\n",
    "        if isinstance(query, list):\n",
    "            q = \" \".join(map(str, query)).lower()\n",
    "        else:\n",
    "            q = str(query).lower()\n",
    "\n",
    "        print(f\"DEBUG: classify_intent received query: '{q}'\")\n",
    "        if re.search(r\"\\b(models|list|available)\\b\", q):\n",
    "            return \"list_models\"\n",
    "        if re.search(r\"\\b(compare|better|best|prefer|efficient|memory|performance)\\b\", q):\n",
    "            return \"compare\"\n",
    "        if re.search(r\"\\b(metrics|performance)\\b\", q):\n",
    "            return \"metrics\"\n",
    "        return \"metrics\"\n",
    "\n",
    "    async def map_query_to_domain_subdomain_llm(self, query, domains, subdomains):\n",
    "\n",
    "        print(\"DEBUG: helper function 'map_query_to_domain_subdomain_llm'\")\n",
    "        lines = []\n",
    "        for d in domains:\n",
    "            subs = [s.lower() for s in subdomains.get(d, [])] or [\"none\"]\n",
    "            lines.append(f\"- {d.lower()}: {', '.join(subs)}\")\n",
    "        prompt = f\"\"\"\n",
    "                You are an AI assistant.\n",
    "                Available domains and subdomains:\n",
    "                {chr(10).join(lines)}\n",
    "                User query: \"{query}\"\n",
    "                The query can map to multiple domains and subdomains. List top 2 closest(if any):\n",
    "                Respond with EXACTLY two lines for each mapping:\n",
    "                Domain: <one domain above>\n",
    "                Subdomain: <one subdomain above or None>\n",
    "                \"\"\"\n",
    "        resp = await self.model.ainvoke([HumanMessage(content=prompt)])\n",
    "        # print(resp)\n",
    "        dm = re.search(r\"Domain:\\s*(\\S+)\", resp.content)\n",
    "        sd = re.search(r\"Subdomain:\\s*(\\S+)\", resp.content)\n",
    "        domain = dm.group(1) if dm else None\n",
    "        sub = sd.group(1).lower() if sd and sd.group(1).lower() != 'none' else None\n",
    "        print(f\"DEBUG: mapping result -> domain={domain}, subdomain={sub}\")\n",
    "        return domain, sub, f\"LLM mapped to {domain}/{sub} or 'None'\"\n",
    "\n",
    "    def normalize_accuracy(self, v):\n",
    "        return v / 100 if isinstance(v, (int, float)) and v > 1 else v\n",
    "\n",
    "    def normalize_metric(self, value, metric_type, all_values):\n",
    "            \"\"\"Normalize a metric to [0,1] based on min-max scaling.\"\"\"\n",
    "            if not all_values:\n",
    "                return 0.0\n",
    "            min_val = min(all_values)\n",
    "            max_val = max(all_values)\n",
    "            if max_val == min_val:\n",
    "                return 1.0  # Avoid division by zero\n",
    "            # For latency and memory, lower is better, so invert the normalization\n",
    "            if metric_type in [\"latency_ms\", \"memory_mb\"]:\n",
    "                return (max_val - value) / (max_val - min_val)\n",
    "            return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "    def select_best_llm(self, metrics, key):\n",
    "        print(\"DEBUG: helper function 'select_best_llm'\")\n",
    "        if not metrics:\n",
    "            return None, \"No metrics data available\"\n",
    "\n",
    "        # Collect all metric values for normalization\n",
    "        metric_values = {\n",
    "            \"accuracy\": [],\n",
    "            \"latency_ms\": [],\n",
    "            \"memory_mb\": []\n",
    "        }\n",
    "        for model_key, data in metrics.items():\n",
    "            if isinstance(data, dict):\n",
    "                for metric in metric_values:\n",
    "                    value = data.get(metric, 0.0)\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        metric_values[metric].append(value)\n",
    "\n",
    "        scores = {}\n",
    "        # print(metrics)\n",
    "        # print(key)\n",
    "        for model_key, data in metrics.items():\n",
    "            if not isinstance(data, dict):\n",
    "                continue\n",
    "            score = 0.0\n",
    "            for metric, weight in self.metric_weights.items():\n",
    "                value = data.get(metric, 0.0)\n",
    "                if isinstance(value, (int, float)) and metric_values[metric]:\n",
    "                    normalized = self.normalize_metric(value, metric, metric_values[metric])\n",
    "                    score += weight * normalized\n",
    "            scores[model_key] = score\n",
    "            print(f\"DEBUG: weighted score: {score} for {model_key}\")\n",
    "\n",
    "\n",
    "        if not scores:\n",
    "            return None, \"No valid metrics for comparison\"\n",
    "\n",
    "        best_model = max(scores.items(), key=lambda x: x[1])[0]\n",
    "        return best_model, f\"Selected {best_model} based on weighted metrics (accuracy: {self.metric_weights['accuracy']}, latency: {self.metric_weights['latency_ms']}, memory: {self.metric_weights['memory_mb']})\"\n",
    "\n",
    "    async def init_step(self, state: AgentState):\n",
    "        print(\"DEBUG: Entering node INIT\")\n",
    "        query = state[\"messages\"][-1].content\n",
    "        intent = self.classify_intent(query)\n",
    "        \n",
    "        print(\"DEBUG: fetching models/domains/subdomains\")\n",
    "        models = await self.tools[\"list_models\"].ainvoke({})\n",
    "        domains = await self.tools[\"list_domains\"].ainvoke({})\n",
    "        subdomains = {}\n",
    "        for d in domains:\n",
    "            subdomains[d] = await self.tools[\"list_sub_domains\"].ainvoke({\"domain\": d})\n",
    "        return {\n",
    "            \"step\": \"init\",\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"models\": models,\n",
    "            \"domains\": domains,\n",
    "            \"subdomains\": subdomains,\n",
    "            \"context\": \"\",\n",
    "            \"metrics\": {},\n",
    "            \"selected_llm\": None,\n",
    "            \"messages\": []\n",
    "        }\n",
    "        \n",
    "    async def collect_metrics_step(self, state: AgentState):\n",
    "        print(\"DEBUG: Entering node collect_metrics_step\")\n",
    "        query = state[\"query\"]\n",
    "        intent = state[\"intent\"]\n",
    "        metrics = {}\n",
    "        messages = []\n",
    "    \n",
    "        domain, subdomain, mapping_reason = await self.map_query_to_domain_subdomain_llm(\n",
    "            query, state[\"domains\"], state[\"subdomains\"]\n",
    "        )\n",
    "        # print(domain, subdomain, mapping_reason)\n",
    "        messages.append(ToolMessage(\n",
    "            tool_call_id=\"mapping\",\n",
    "            name=\"mapping\",\n",
    "            content=mapping_reason\n",
    "        ))\n",
    "    \n",
    "        def parse_tool_result(raw):\n",
    "            try:\n",
    "                if isinstance(raw, dict):\n",
    "                    data = raw\n",
    "                elif isinstance(raw, str):\n",
    "                    data = json.loads(raw)\n",
    "                else:\n",
    "                    data = {}\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(raw)\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "        \n",
    "            if isinstance(data, dict) and 'accuracy' in data:\n",
    "                data['accuracy'] = self.normalize_accuracy(data['accuracy'])\n",
    "            return data\n",
    "\n",
    "        if intent == \"metrics\" and domain:\n",
    "            api_domain = domain.title()\n",
    "            api_sub = subdomain.lower() if subdomain else None\n",
    "    \n",
    "            for model_name in state[\"models\"]:\n",
    "                print(f\"DEBUG: Calling metrics for {model_name} in {api_domain}/{api_sub}\")\n",
    "                try:\n",
    "                    if api_sub:\n",
    "                        raw = await self.tools[\"get_metric_domain_subdomain\"].ainvoke({\n",
    "                            \"model\": model_name,\n",
    "                            \"domain\": api_domain,\n",
    "                            \"sub_domain\": api_sub,\n",
    "                        })\n",
    "                    else:\n",
    "                        raw = await self.tools[\"get_metric_domain\"].ainvoke({\n",
    "                            \"model\": model_name,\n",
    "                            \"domain\": api_domain,\n",
    "                        })\n",
    "                    # print(f\"Raw output for {model_name}:\\n{raw}\\n---\")\n",
    "        \n",
    "                    data = parse_tool_result(raw)\n",
    "                    print(f\"DEBUG: Printing data for {model_name}\\n\")\n",
    "                    print(data)\n",
    "                    print(\"\\n\")\n",
    "                    key = f\"{model_name}_{api_domain}_{api_sub}\" if api_sub else f\"{model_name}_{api_domain}\"\n",
    "                    metrics[key] = data\n",
    "                    messages.append(ToolMessage(\n",
    "                        tool_call_id=f\"metrics_{key}\",\n",
    "                        name=(\"get_metric_domain_subdomain\" if api_sub else \"get_metric_domain\"),\n",
    "                        content=str(data)\n",
    "                    ))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching metrics for {model_name}: {e}\")\n",
    "            try:\n",
    "                best_model, reason = self.select_best_llm(metrics, api_sub or api_domain)\n",
    "                print(f\"DEBUG: Best model: {best_model}\\nReason:{reason}\")\n",
    "                state[\"selected_llm\"] = best_model\n",
    "                state[\"context\"] = reason\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting best model: {e}\")\n",
    "                state[\"selected_llm\"] = None\n",
    "                state[\"context\"] = \"Error during model selection.\"\n",
    "    \n",
    "        return {\n",
    "            \"step\": \"metrics\",\n",
    "            \"metrics\": metrics,\n",
    "            \"messages\": messages,\n",
    "            \"selected_llm\": state.get(\"selected_llm\"),\n",
    "            \"context\": state.get(\"context\")\n",
    "        }\n",
    "\n",
    "    async def respond_step(self, state: AgentState):\n",
    "        print(\"DEBUG: Entering node respond_step\\n\")\n",
    "        raw_q = state.get(\"query\")\n",
    "        if isinstance(raw_q, list):\n",
    "            query = \" \".join(str(x) for x in raw_q).lower()\n",
    "        else:\n",
    "            query = str(raw_q or \"\").lower()\n",
    "    \n",
    "        intent = state.get(\"intent\")\n",
    "        metrics = state.get(\"metrics\", {})\n",
    "        context = state.get(\"context\", \"\")\n",
    "        selected_llm = state.get(\"selected_llm\")\n",
    "        # print(metrics)\n",
    "        print(f\"Responding with intent='{intent}', context={context}, selected_llm={selected_llm}, metrics={metrics}\")\n",
    "        response = f\"Based on the analysis and context:\\nContext: {context}\\n\\n\"\n",
    "        if intent == \"list_models\" and metrics.get(\"models\"):\n",
    "            response += f\"Available models: {', '.join(metrics['models'])}\\n\"\n",
    "        elif intent in [\"compare\", \"metrics\"]:\n",
    "            if intent == \"compare\" and metrics.get(\"comparison\"):\n",
    "                comp = metrics[\"comparison\"]\n",
    "                best, best_acc = max(comp.items(), key=lambda kv: kv[1])\n",
    "                response += f\"Best model: {best} (accuracy={best_acc:.2f})\\n\"\n",
    "                response += \"Model accuracies:\\n\" + \"\\n\".join(f\"{m}: {a:.2f}\" for m, a in comp.items()) + \"\\n\"\n",
    "            elif intent == \"metrics\" and metrics:\n",
    "                response += \"Metrics collected for models:\\n\"\n",
    "                for m, v in metrics.items():\n",
    "                    response += f\"{m}:\\n\"\n",
    "                    for metric_name, value in v.items():\n",
    "                        response += f\"  {metric_name}: {value}\\n\"\n",
    "                    response += \"\\n\"\n",
    "                    # print(\"DEBUG: I am printing the response now....\")\n",
    "                    # print(response)\n",
    "            else:\n",
    "                response += \"No data found for this query.\\n\"\n",
    "            \n",
    "        if selected_llm:\n",
    "            model_parts = selected_llm.split(\"_\")\n",
    "            model_name = \"_\".join(model_parts[:2])\n",
    "            print(f\"\\nModel: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcf29429-5056-4675-ac6d-b4cbd119e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Entering node INIT\n",
      "DEBUG: helper function 'classify_intent'\n",
      "DEBUG: classify_intent received query: 'please respond the answers related to historical geometry?'\n",
      "DEBUG: fetching models/domains/subdomains\n",
      "DEBUG: Entering node collect_metrics_step\n",
      "DEBUG: helper function 'map_query_to_domain_subdomain_llm'\n",
      "DEBUG: mapping result -> domain=mathematics, subdomain=geometry\n",
      "DEBUG: Calling metrics for Llama3.2_1B in Mathematics/geometry\n",
      "DEBUG: Printing data for Llama3.2_1B\n",
      "\n",
      "{'accuracy': 0.55, 'latency_ms': 7466.44, 'memory_mb': 0.0, 'cpu_ms': 1.2904, 'total_duration_ms': 7466.44, 'load_duration_ms': 91.056, 'prompt_eval_count': 52.875, 'prompt_eval_duration_ms': 417.27, 'eval_count': 149.275, 'eval_duration_ms': 6958.106}\n",
      "\n",
      "\n",
      "DEBUG: Calling metrics for Qwen2.5_1.5B in Mathematics/geometry\n",
      "DEBUG: Printing data for Qwen2.5_1.5B\n",
      "\n",
      "{'accuracy': 0.675, 'latency_ms': 14657.977, 'memory_mb': 0.0, 'cpu_ms': 1.642, 'total_duration_ms': 14649.94, 'load_duration_ms': 69.45, 'prompt_eval_count': 58.0, 'prompt_eval_duration_ms': 877.0, 'eval_count': 269.6, 'eval_duration_ms': 13703.12}\n",
      "\n",
      "\n",
      "DEBUG: helper function 'select_best_llm'\n",
      "DEBUG: weighted score: 0.30000000000000004 for Llama3.2_1B_Mathematics_geometry\n",
      "DEBUG: weighted score: 0.7999999999999999 for Qwen2.5_1.5B_Mathematics_geometry\n",
      "DEBUG: Best model: Qwen2.5_1.5B_Mathematics_geometry\n",
      "Reason:Selected Qwen2.5_1.5B_Mathematics_geometry based on weighted metrics (accuracy: 0.7, latency: 0.2, memory: 0.1)\n",
      "DEBUG: Entering node respond_step\n",
      "\n",
      "Responding with intent='metrics', context=Selected Qwen2.5_1.5B_Mathematics_geometry based on weighted metrics (accuracy: 0.7, latency: 0.2, memory: 0.1), selected_llm=Qwen2.5_1.5B_Mathematics_geometry, metrics={'Llama3.2_1B_Mathematics_geometry': {'accuracy': 0.55, 'latency_ms': 7466.44, 'memory_mb': 0.0, 'cpu_ms': 1.2904, 'total_duration_ms': 7466.44, 'load_duration_ms': 91.056, 'prompt_eval_count': 52.875, 'prompt_eval_duration_ms': 417.27, 'eval_count': 149.275, 'eval_duration_ms': 6958.106}, 'Qwen2.5_1.5B_Mathematics_geometry': {'accuracy': 0.675, 'latency_ms': 14657.977, 'memory_mb': 0.0, 'cpu_ms': 1.642, 'total_duration_ms': 14649.94, 'load_duration_ms': 69.45, 'prompt_eval_count': 58.0, 'prompt_eval_duration_ms': 877.0, 'eval_count': 269.6, 'eval_duration_ms': 13703.12}}\n",
      "\n",
      "Model: Qwen2.5_1.5B\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an Intelligent router responsible for handling queries within a Multi-Agent System (MAS) framework.\n",
    "\n",
    "Upon receiving a query from the MAS, your task is to:\n",
    "1. Accurately classify the query's intent.\n",
    "2. Identify the relevant domain(s) and subdomain(s) associated with the query.\n",
    "3. Utilize available tool calls through the MCP server to:\n",
    "   - Retrieve necessary information such as available models, domains, and subdomains.\n",
    "   - Fetch and normalize model performance metrics.\n",
    "   - Select the most suitable LLM(s) capable of addressing the query effectively.\n",
    "4. If the query spans multiple domains or subdomains, you are allowed to make multiple tool calls to ensure optimal model selection.\n",
    "5. Your response must only contain the selected model's name in the following format: \"Model:___\"\n",
    "   \n",
    "\"\"\"\n",
    "async def main():\n",
    "    tools = await client.get_tools()\n",
    "    model = ChatTogether(model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", temperature=0.0)\n",
    "    agent = Agent(model, tools, checkpointer=memory,system=prompt)\n",
    "\n",
    "    query = [\n",
    "    \"please respond the answers related to historical geometry?\"\n",
    "    # \"Please respond the answers related to titration experiment?\"\n",
    "        # \"Please respond the answers related to : My laptop keeps crashing when I open the video editing software!\"\n",
    "    ]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"step\": \"init\",\n",
    "        \"models\": [],\n",
    "        \"domains\": [],\n",
    "        \"subdomains\": {},\n",
    "        \"query\": query,\n",
    "        \"intent\": \"\",\n",
    "        \"metrics\": {},\n",
    "        \"context\": \"\",\n",
    "        \"selected_llm\": None\n",
    "    }\n",
    "    result = await agent.graph.ainvoke(state, thread)\n",
    "    # print(result[\"messages\"][-1].content)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
