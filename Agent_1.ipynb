{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1785cfc-62b9-4fb8-bccc-3131f7382177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_mcp_adapters.tools import to_fastmcp\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated,List\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.tools import tool\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502bd47-f2ce-4571-a7d3-ac15e6b95d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d63356-9ece-4c5f-bb4c-c291f7651283",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "print(f\"Checkpointer type: {type(memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f242ed-5b9d-4dd0-81bc-2fcf7d7a8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"csv_r\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\":  [\n",
    "                \"--directory\",\n",
    "                \"/home/sesi/testing/agents/ER/agentic-ai/experiments/HeteroLLMs/mcp_server/\",\n",
    "                \"run\",\n",
    "                \"main.py\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6070186-41b2-4a34-8d9c-604f7f669fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_llm)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_llm(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    async def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = await self.tools[t['name']].ainvoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6207f2-a94a-4583-bc7f-977ed1e6c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI assistant in a Multi-Agent System (MAS) framework, tasked with dynamically selecting the most suitable LLM for a given task using only the provided CSV dataset tools (Model, Parameters, Domain, Sub-domain, Accuracy, Latency_ms, Memory_mb, etc.). The user query can be in any text format. Follow this workflow:\n",
    "1. Classify the query intent (e.g., list models, compare models, retrieve metrics).\n",
    "2. Check available domains and subdomains from the CSV at the start.\n",
    "3. Dynamically map the query to the closest domain-subdomain pair by comparing query terms to available domains/subdomains, using keyword overlap or similarity, without hardcoded mappings.\n",
    "4. Use cached data to avoid redundant tool calls.\n",
    "5. Evaluate metrics (e.g., accuracy for knowledge tasks) to select the best LLM at each step.\n",
    "6. Normalize accuracy to 0-1 scale if >1.\n",
    "7. Provide a clear explanation of the selected LLM, including metrics, why it was chosen, and the domain-subdomain mapping, referencing context if relevant.\n",
    "8. If no data is found, respond: \"No specific data found for this query in the CSV. Try a related domain/subdomain or check the query.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Use only the CSV dataset tools. Log decisions for debugging. Select the best LLM at each step where metrics are available.\n",
    "\"\"\"\n",
    "\n",
    "tools = await client.get_tools()\n",
    "model= ChatTogether(\n",
    "    # model_name=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    temperature=0.0\n",
    ") \n",
    "\n",
    "agent = Agent(model, tools, checkpointer=memory, system=prompt)\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}} #Unique thread identifier for the conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c4428-a41c-4d02-8d55-bea6ec5c9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"what models are listed in the csv file available in the tools?.\")]\n",
    "result = await agent.graph.ainvoke({\"messages\": messages}, thread)\n",
    "print(result['messages'][-1].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebaee4-8713-464f-b58e-0cc84cd97968",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"which LLM performs better in history( General-Knowledge domain) related queries? and why? show the metrics also only for history\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}} \n",
    "result = await agent.graph.ainvoke({\"messages\": messages},thread)\n",
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5858355-99bc-44d3-a418-1bddb6262b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"which LLM would you prefer for tasks related to astronomical geometrical distances calculations? \")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}} #existing conversation thread\n",
    "result = await agent.graph.ainvoke({\"messages\": messages},thread)\n",
    "result['messages'][-1].content\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
