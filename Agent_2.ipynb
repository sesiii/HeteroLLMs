{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20fea6-66c7-4ca7-863d-1b2b54a12a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain_fireworks import ChatFireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb04713-709d-4cce-8c0c-d1ca946f6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    step: str  # Tracks current step: 'init', 'models', 'domains', 'subdomains', 'metrics', 'done'\n",
    "    models: list[str]\n",
    "    domains: list[str]\n",
    "    subdomains: dict[str, list[str]]\n",
    "    query: str\n",
    "    metrics: dict\n",
    "    intent: str\n",
    "    context: str\n",
    "    selected_llm: str\n",
    "\n",
    "# Initialize memory and MCP client\n",
    "memory = MemorySaver()\n",
    "print(f\"Checkpointer type: {type(memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151f4d5-e589-4e40-92e4-b28de8c4afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCP client\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"csv_r\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"--directory\",\n",
    "                \"/home/sesi/testing/agents/ER/agentic-ai/experiments/HeteroLLMs/mcp_server/\",\n",
    "                \"run\",\n",
    "                \"main.py\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0adf40-e9a4-4ef4-85b2-fc5f54fe9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI assistant in a Multi-Agent System (MAS) framework, tasked with dynamically selecting the most suitable LLM for a given task using only the provided CSV dataset tools (Model, Parameters, Domain, Sub-domain, Accuracy, Latency_ms, Memory_mb, etc.). The user query can be in any text format. Follow this workflow:\n",
    "1. Classify the query intent (e.g., list models, compare models, retrieve metrics).\n",
    "2. Check available domains and subdomains from the CSV at the start.\n",
    "3. Dynamically map the query to the closest domain-subdomain pair by comparing query terms to available domains/subdomains, using keyword overlap or similarity, without hardcoded mappings.\n",
    "4. Use cached data to avoid redundant tool calls.\n",
    "5. Evaluate metrics (e.g., accuracy for knowledge tasks) to select the best LLM at each step.\n",
    "6. Normalize accuracy to 0-1 scale if >1.\n",
    "7. Provide a clear explanation of the selected LLM, including metrics, why it was chosen, and the domain-subdomain mapping, referencing context if relevant.\n",
    "8. If no data is found, respond: \"No specific data found for this query in the CSV. Try a related domain/subdomain or check the query.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Use only the \"csv_r\" MCP server tools. Log decisions for debugging. Select the best LLM at each step where metrics are available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d42026e-80e3-4aab-b0d7-65c0f45900b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer):\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "        self.graph = self._build_graph(checkpointer)\n",
    "\n",
    "    def _build_graph(self, checkpointer):\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"init\", self.init_step)\n",
    "        graph.add_node(\"list_models\", self.list_models_step)\n",
    "        graph.add_node(\"list_domains\", self.list_domains_step)\n",
    "        graph.add_node(\"list_subdomains\", self.list_subdomains_step)\n",
    "        graph.add_node(\"collect_metrics\", self.collect_metrics_step)\n",
    "        graph.add_node(\"respond\", self.respond_step)\n",
    "        graph.add_conditional_edges(\n",
    "            \"init\",\n",
    "            lambda state: self.route_step(state),\n",
    "            {\"list_models\": \"list_models\", \"list_domains\": \"list_domains\", \"collect_metrics\": \"collect_metrics\", \"respond\": \"respond\"}\n",
    "        )\n",
    "        graph.add_conditional_edges(\n",
    "            \"list_models\",\n",
    "            lambda state: \"list_domains\" if state[\"intent\"] in [\"compare\", \"metrics\"] and not state[\"domains\"] else \"collect_metrics\",\n",
    "            {\"list_domains\": \"list_domains\", \"collect_metrics\": \"collect_metrics\"}\n",
    "        )\n",
    "        graph.add_edge(\"list_domains\", \"list_subdomains\")\n",
    "        graph.add_edge(\"list_subdomains\", \"collect_metrics\")\n",
    "        graph.add_edge(\"collect_metrics\", \"respond\")\n",
    "        graph.set_entry_point(\"init\")\n",
    "        return graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def route_step(self, state: AgentState):\n",
    "        \"\"\"Dynamically route based on intent and cached data.\"\"\"\n",
    "        intent = state[\"intent\"]\n",
    "        print(f\"Routing: Intent={intent}, Models={state['models']}, Domains={state['domains']}, Subdomains={state['subdomains']}\")\n",
    "        if intent == \"list_models\" and state[\"models\"]:\n",
    "            print(\"Routing: Using cached models, skipping list_models\")\n",
    "            return \"collect_metrics\" if state[\"intent\"] in [\"compare\", \"metrics\"] else \"respond\"\n",
    "        elif intent == \"list_models\":\n",
    "            return \"list_models\"\n",
    "        elif intent in [\"compare\", \"metrics\"] and state[\"models\"] and state[\"domains\"] and state[\"subdomains\"]:\n",
    "            print(\"Routing: Using cached models, domains, and subdomains\")\n",
    "            return \"collect_metrics\"\n",
    "        elif intent in [\"compare\", \"metrics\"]:\n",
    "            return \"list_models\"\n",
    "        return \"respond\"\n",
    "\n",
    "    async def init_step(self, state: AgentState):\n",
    "        query = state[\"messages\"][-1].content\n",
    "        print(f\"Initializing with query: {query}\")\n",
    "        intent = self.classify_intent(query)\n",
    "        print(f\"Classified intent: {intent}\")\n",
    "\n",
    "        # Initialize with cached data or fetch from tools\n",
    "        models = state.get(\"models\", [])\n",
    "        domains = state.get(\"domains\", [])\n",
    "        subdomains = state.get(\"subdomains\", {})\n",
    "        if not models and intent in [\"list_models\", \"compare\", \"metrics\"]:\n",
    "            models_result = await self.tools[\"list_models\"].ainvoke({})\n",
    "            models = models_result if isinstance(models_result, list) else []\n",
    "            print(f\"Models retrieved: {models}\")\n",
    "        if not domains and intent in [\"compare\", \"metrics\"]:\n",
    "            domains_result = await self.tools[\"list_domains\"].ainvoke({})\n",
    "            domains = domains_result if isinstance(domains_result, list) else []\n",
    "            print(f\"Domains retrieved: {domains}\")\n",
    "        if not subdomains and intent in [\"compare\", \"metrics\"]:\n",
    "            subdomains = {}\n",
    "            for domain in domains:\n",
    "                subdomains_result = await self.tools[\"list_sub_domains\"].ainvoke({\"domain\": domain})\n",
    "                subdomains[domain] = subdomains_result if isinstance(subdomains_result, list) else []\n",
    "                print(f\"Subdomains for {domain}: {subdomains[domain]}\")\n",
    "            print(f\"Collected subdomains: {subdomains}\")\n",
    "\n",
    "        context = await self.summarize_context(state[\"messages\"], models, domains, subdomains)\n",
    "        print(f\"Context: {context}\")\n",
    "        return {\n",
    "            \"step\": \"init\",\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"context\": context,\n",
    "            \"models\": models,\n",
    "            \"domains\": domains,\n",
    "            \"subdomains\": subdomains,\n",
    "            \"metrics\": {},\n",
    "            \"selected_llm\": None,\n",
    "            \"messages\": []\n",
    "        }\n",
    "\n",
    "    def classify_intent(self, query: str) -> str:\n",
    "        q = query.lower()\n",
    "        if re.search(r\"\\b(models|list|available)\\b\", q):\n",
    "            return \"list_models\"\n",
    "        if re.search(r\"\\b(compare|better|best|prefer|efficient|memory|performance)\\b\", q):\n",
    "            return \"compare\"\n",
    "        if re.search(r\"\\b(metrics|performance)\\b\", q):\n",
    "            return \"metrics\"\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "    async def summarize_context(self, messages: list[AnyMessage], models: list, domains: list, subdomains: dict) -> str:\n",
    "        \"\"\"Summarize previous responses and available data for context.\"\"\"\n",
    "        context_lines = [f\"Available models: {', '.join(models) if models else 'None'}\",\n",
    "                         f\"Available domains: {', '.join(domains) if domains else 'None'}\",\n",
    "                         f\"Available subdomains: {subdomains if subdomains else 'None'}\"]\n",
    "        seen_outputs = set()\n",
    "        for msg in messages[-5:]:\n",
    "            if isinstance(msg, AIMessage):\n",
    "                if \"Available models\" in msg.content:\n",
    "                    models_match = re.search(r\"Available models: (.+)\", msg.content)\n",
    "                    if models_match and models_match.group(1) not in seen_outputs:\n",
    "                        context_lines.append(f\"Models: {models_match.group(1)}\")\n",
    "                        seen_outputs.add(models_match.group(1))\n",
    "                elif \"The best model\" in msg.content:\n",
    "                    best_model = re.search(r\"The best model for the query is (\\S+) with an average accuracy of (\\d+\\.\\d+)\", msg.content)\n",
    "                    if best_model and best_model.group(0) not in seen_outputs:\n",
    "                        context_lines.append(f\"Best model: {best_model.group(1)} (accuracy: {best_model.group(2)})\")\n",
    "                        seen_outputs.add(best_model.group(0))\n",
    "                elif \"Metrics for\" in msg.content:\n",
    "                    metrics = re.findall(r\"Metrics for (\\S+):\\n(.*?)(?=\\n\\n|$)\", msg.content, re.DOTALL)\n",
    "                    for metric in metrics:\n",
    "                        key = metric[0]\n",
    "                        if key not in seen_outputs:\n",
    "                            context_lines.append(f\"Metrics for {key}: {metric[1].strip()}\")\n",
    "                            seen_outputs.add(key)\n",
    "            elif isinstance(msg, ToolMessage) and msg.name in [\"list_models\", \"compare_models_domain_subdomain\", \"get_metric_domain_subdomain\"]:\n",
    "                if msg.content not in seen_outputs:\n",
    "                    context_lines.append(f\"{msg.name}: {msg.content}\")\n",
    "                    seen_outputs.add(msg.content)\n",
    "        context = \"\\n\".join(context_lines)\n",
    "        summary_prompt = f\"Summarize this context in 100 words or less, focusing on key information (models, best models, metrics, domains, subdomains):\\n{context}\"\n",
    "        try:\n",
    "            summary = await self.model.ainvoke([HumanMessage(content=summary_prompt)])\n",
    "            return summary.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing context: {e}\")\n",
    "            return context[:500] or \"No previous context available.\"\n",
    "\n",
    "    async def list_models_step(self, state: AgentState):\n",
    "        try:\n",
    "            result = await self.tools[\"list_models\"].ainvoke({})\n",
    "            print(f\"Models retrieved: {result}, Type: {type(result)}\")\n",
    "            return {\n",
    "                \"step\": \"models\",\n",
    "                \"models\": result if isinstance(result, list) else [],\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=\"list_models_1\",\n",
    "                    name=\"list_models\",\n",
    "                    content=str(result)\n",
    "                )]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in list_models: {e}\")\n",
    "            return {\n",
    "                \"step\": \"models\",\n",
    "                \"models\": [],\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=\"list_models_1\",\n",
    "                    name=\"list_models\",\n",
    "                    content=f\"Error: {e}\"\n",
    "                )]\n",
    "            }\n",
    "\n",
    "    async def list_domains_step(self, state: AgentState):\n",
    "        try:\n",
    "            result = await self.tools[\"list_domains\"].ainvoke({})\n",
    "            print(f\"Domains retrieved: {result}, Type: {type(result)}\")\n",
    "            return {\n",
    "                \"step\": \"domains\",\n",
    "                \"domains\": result if isinstance(result, list) else [],\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=\"list_domains_1\",\n",
    "                    name=\"list_domains\",\n",
    "                    content=str(result)\n",
    "                )]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in list_domains: {e}\")\n",
    "            return {\n",
    "                \"step\": \"domains\",\n",
    "                \"domains\": [],\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=\"list_domains_1\",\n",
    "                    name=\"list_domains\",\n",
    "                    content=f\"Error: {e}\"\n",
    "                )]\n",
    "            }\n",
    "\n",
    "    async def list_subdomains_step(self, state: AgentState):\n",
    "        subdomains = {}\n",
    "        try:\n",
    "            for domain in state[\"domains\"]:\n",
    "                result = await self.tools[\"list_sub_domains\"].ainvoke({\"domain\": domain})\n",
    "                print(f\"Sub-domains for {domain}: {result}, Type: {type(result)}\")\n",
    "                subdomains[domain] = result if isinstance(result, list) else []\n",
    "            print(f\"Collected subdomains: {subdomains}\")\n",
    "            return {\n",
    "                \"step\": \"subdomains\",\n",
    "                \"subdomains\": subdomains,\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=f\"list_subdomains_{domain}\",\n",
    "                    name=\"list_sub_domains\",\n",
    "                    content=str(subdomains.get(domain, []))\n",
    "                ) for domain in subdomains]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in list_subdomains: {e}\")\n",
    "            return {\n",
    "                \"step\": \"subdomains\",\n",
    "                \"subdomains\": {},\n",
    "                \"messages\": [ToolMessage(\n",
    "                    tool_call_id=\"list_subdomains_error\",\n",
    "                    name=\"list_sub_domains\",\n",
    "                    content=f\"Error: {e}\"\n",
    "                )]\n",
    "            }\n",
    "\n",
    "    def normalize_accuracy(self, value):\n",
    "        \"\"\"Normalize accuracy to 0-1 scale if needed.\"\"\"\n",
    "        if isinstance(value, (int, float)) and value > 1:\n",
    "            return value / 100\n",
    "        return value\n",
    "\n",
    "    async def map_query_to_domain_subdomain_llm(self, query, domains, subdomains):\n",
    "        # build prompt listing each domain + its subdomains\n",
    "        dom_list = []\n",
    "        for d in domains:\n",
    "            subs = subdomains.get(d, []) or [\"None\"]\n",
    "            dom_list.append(f\"- {d}: {', '.join(subs)}\")\n",
    "        prompt = f\"\"\"\n",
    "    You are given these domains/subdomains:\n",
    "    {chr(10).join(dom_list)}\n",
    "    User query: \"{query}\"\n",
    "    Reply with exactly:\n",
    "    Domain: <one of the domains above>\n",
    "    Subdomain: <one of that domainâ€™s subdomains, or None>\n",
    "    \"\"\"\n",
    "        resp = await self.model.ainvoke([HumanMessage(content=prompt)])\n",
    "        dm = re.search(r\"Domain:\\s*(\\S+)\", resp.content)\n",
    "        sd = re.search(r\"Subdomain:\\s*(\\S+)\", resp.content)\n",
    "        domain = dm.group(1) if dm else None\n",
    "        subdomain = sd.group(1) if sd and sd.group(1) != 'None' else None\n",
    "        reason = f\"LLM mapped to {domain}/{subdomain or 'None'}\"\n",
    "        return domain, subdomain, reason\n",
    "\n",
    "\n",
    "    def select_best_llm(self, metrics: dict, task_type: str) -> tuple[str, str]:\n",
    "        \"\"\"Select the best LLM based on task type and metrics.\"\"\"\n",
    "        if not metrics or \"comparison\" not in metrics:\n",
    "            return None, \"No comparison data available.\"\n",
    "        comparison = metrics[\"comparison\"]\n",
    "        if not isinstance(comparison, dict) or not comparison:\n",
    "            return None, f\"Invalid comparison data: {comparison}\"\n",
    "        best_model = max(comparison.items(), key=lambda x: x[1], default=(None, 0))[0]\n",
    "        reason = f\"Selected {best_model} due to highest accuracy ({comparison[best_model]:.2f}) for {task_type}.\"\n",
    "        return best_model, reason\n",
    "\n",
    "    async def collect_metrics_step(self, state: AgentState):\n",
    "        query = state[\"query\"].lower()\n",
    "        intent = state[\"intent\"]\n",
    "        metrics = {}\n",
    "        messages = []\n",
    "        selected_llm = None\n",
    "        selection_reason = \"\"\n",
    "    \n",
    "# inside collect_metrics_step\n",
    "        relevant_domain, relevant_subdomain, mapping_reason = await \\\n",
    "            self.map_query_to_domain_subdomain_llm(query, state[\"domains\"], state[\"subdomains\"])\n",
    "\n",
    "        messages.append(ToolMessage(\n",
    "            tool_call_id=\"mapping_decision\",\n",
    "            name=\"mapping\",\n",
    "            content=mapping_reason\n",
    "        ))\n",
    "    \n",
    "        # 2) Helper to parse & normalize\n",
    "        def parse_tool_result(res):\n",
    "            try:\n",
    "                data = res if isinstance(res, dict) else json.loads(res)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    data = ast.literal_eval(res)\n",
    "                except Exception:\n",
    "                    return {}\n",
    "            if isinstance(data, dict) and \"accuracy\" in data:\n",
    "                data[\"accuracy\"] = self.normalize_accuracy(data[\"accuracy\"])\n",
    "            return data\n",
    "    \n",
    "        # 3) Invoke the correct tool based on intent\n",
    "        try:\n",
    "            if intent == \"list_models\":\n",
    "                metrics[\"models\"] = state[\"models\"]\n",
    "                messages.append(ToolMessage(\n",
    "                    tool_call_id=\"list_models_response\",\n",
    "                    name=\"list_models\",\n",
    "                    content=str(state[\"models\"])\n",
    "                ))\n",
    "    \n",
    "            elif intent in [\"compare\", \"metrics\"]:\n",
    "                # comparison request\n",
    "                if re.search(r\"\\b(better|best|prefer|efficient|memory)\\b\", query) and domain:\n",
    "                    if subdomain:\n",
    "                        cmp_res = await self.tools[\"compare_models_domain_subdomain\"].ainvoke(\n",
    "                            {\"domain\": domain, \"sub_domain\": subdomain}\n",
    "                        )\n",
    "                    else:\n",
    "                        cmp_res = await self.tools[\"compare_models_domain\"].ainvoke(\n",
    "                            {\"domain\": domain}\n",
    "                        )\n",
    "                    comp = parse_tool_result(cmp_res)\n",
    "                    if isinstance(comp, dict):\n",
    "                        comp = {m: self.normalize_accuracy(a) for m, a in comp.items()}\n",
    "                    metrics[\"comparison\"] = comp\n",
    "                    messages.append(ToolMessage(\n",
    "                        tool_call_id=f\"compare_{domain}_{subdomain or 'all'}\",\n",
    "                        name=(\"compare_models_domain_subdomain\"\n",
    "                              if subdomain else \"compare_models_domain\"),\n",
    "                        content=str(comp)\n",
    "                    ))\n",
    "                    # pick best\n",
    "                    selected_llm, selection_reason = self.select_best_llm(metrics, subdomain or domain)\n",
    "    \n",
    "                # detailed metrics request\n",
    "                if re.search(r\"\\bmetrics\\b\", query) and domain:\n",
    "                    for m in state[\"models\"]:\n",
    "                        if subdomain:\n",
    "                            res = await self.tools[\"get_metric_domain_subdomain\"].ainvoke(\n",
    "                                {\"model\": m, \"domain\": domain, \"sub_domain\": subdomain}\n",
    "                            )\n",
    "                        else:\n",
    "                            res = await self.tools[\"get_metric_domain\"].ainvoke(\n",
    "                                {\"model\": m, \"domain\": domain}\n",
    "                            )\n",
    "                        parsed = parse_tool_result(res)\n",
    "                        key = f\"{m}_{domain}_{subdomain}\" if subdomain else f\"{m}_{domain}\"\n",
    "                        metrics[key] = parsed\n",
    "                        messages.append(ToolMessage(\n",
    "                            tool_call_id=f\"metrics_{key}\",\n",
    "                            name=(\"get_metric_domain_subdomain\"\n",
    "                                  if subdomain else \"get_metric_domain\"),\n",
    "                            content=str(parsed)\n",
    "                        ))\n",
    "        except Exception as e:\n",
    "            messages.append(ToolMessage(\n",
    "                tool_call_id=\"metrics_error\",\n",
    "                name=\"metrics\",\n",
    "                content=f\"Error: {e}\"\n",
    "            ))\n",
    "    \n",
    "        # 4) Build the next state\n",
    "        return {\n",
    "            \"step\": \"metrics\",\n",
    "            \"metrics\": metrics,\n",
    "            \"messages\": messages,\n",
    "            \"selected_llm\": selected_llm,\n",
    "            \"context\": (\n",
    "                state[\"context\"] +\n",
    "                (f\"\\nSelected LLM: {selected_llm} ({selection_reason})\" if selected_llm else \"\")\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "        def parse_tool_result(result):\n",
    "            if isinstance(result, dict):\n",
    "                if \"accuracy\" in result:\n",
    "                    result[\"accuracy\"] = self.normalize_accuracy(result[\"accuracy\"])\n",
    "                return result\n",
    "            elif isinstance(result, str):\n",
    "                try:\n",
    "                    parsed = json.loads(result)\n",
    "                    if isinstance(parsed, dict) and \"accuracy\" in parsed:\n",
    "                        parsed[\"accuracy\"] = self.normalize_accuracy(parsed[\"accuracy\"])\n",
    "                    return parsed\n",
    "                except json.JSONDecodeError:\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(result)\n",
    "                        if isinstance(parsed, dict) and \"accuracy\" in parsed:\n",
    "                            parsed[\"accuracy\"] = self.normalize_accuracy(parsed[\"accuracy\"])\n",
    "                        return parsed\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        print(f\"Warning: Failed to parse string result: {result}\")\n",
    "                        return {}\n",
    "            elif isinstance(result, list):\n",
    "                print(f\"Warning: Tool returned a list: {result}\")\n",
    "                return {}\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected result type: {type(result)}, value: {result}\")\n",
    "                return {}\n",
    "\n",
    "        try:\n",
    "            if intent == \"list_models\":\n",
    "                metrics[\"models\"] = state[\"models\"]\n",
    "                messages.append(ToolMessage(\n",
    "                    tool_call_id=\"list_models_response\",\n",
    "                    name=\"list_models\",\n",
    "                    content=str(state[\"models\"])\n",
    "                ))\n",
    "            elif intent in [\"compare\", \"metrics\"]:\n",
    "                if \"better\" in query or \"prefer\" in query or \"best\" in query:\n",
    "                    if relevant_domain and relevant_subdomain:\n",
    "                        result = await self.tools[\"compare_models_domain_subdomain\"].ainvoke({\n",
    "                            \"domain\": relevant_domain,\n",
    "                            \"sub_domain\": relevant_subdomain\n",
    "                        })\n",
    "                        result = parse_tool_result(result)\n",
    "                        if isinstance(result, dict):\n",
    "                            result = {k: self.normalize_accuracy(v) for k, v in result.items()}\n",
    "                        print(f\"Comparison result for {relevant_domain}/{relevant_subdomain}: {result}\")\n",
    "                        metrics[\"comparison\"] = result\n",
    "                        messages.append(ToolMessage(\n",
    "                            tool_call_id=f\"compare_models_{relevant_domain}_{relevant_subdomain}\",\n",
    "                            name=\"compare_models_domain_subdomain\",\n",
    "                            content=str(result)\n",
    "                        ))\n",
    "                        task_type = relevant_subdomain or relevant_domain or query\n",
    "                        selected_llm, selection_reason = self.select_best_llm(metrics, task_type)\n",
    "                    elif relevant_domain:\n",
    "                        result = await self.tools[\"compare_models_domain\"].ainvoke({\"domain\": relevant_domain})\n",
    "                        result = parse_tool_result(result)\n",
    "                        if isinstance(result, dict):\n",
    "                            result = {k: self.normalize_accuracy(v) for k, v in result.items()}\n",
    "                        print(f\"Comparison result for {relevant_domain}: {result}\")\n",
    "                        metrics[\"comparison\"] = result\n",
    "                        messages.append(ToolMessage(\n",
    "                            tool_call_id=f\"compare_models_{relevant_domain}\",\n",
    "                            name=\"compare_models_domain\",\n",
    "                            content=str(result)\n",
    "                        ))\n",
    "                        task_type = relevant_domain or query\n",
    "                        selected_llm, selection_reason = self.select_best_llm(metrics, task_type)\n",
    "                if \"metrics\" in query and relevant_domain:\n",
    "                    for model in state[\"models\"]:\n",
    "                        if relevant_subdomain:\n",
    "                            result = await self.tools[\"get_metric_domain_subdomain\"].ainvoke({\n",
    "                                \"model\": model,\n",
    "                                \"domain\": relevant_domain,\n",
    "                                \"sub_domain\": relevant_subdomain\n",
    "                            })\n",
    "                            result = parse_tool_result(result)\n",
    "                            print(f\"Metrics for {model}/{relevant_domain}/{relevant_subdomain}: {result}\")\n",
    "                            metrics[f\"{model}_{relevant_domain}_{relevant_subdomain}\"] = result\n",
    "                            messages.append(ToolMessage(\n",
    "                                tool_call_id=f\"metrics_{model}_{relevant_domain}_{relevant_subdomain}\",\n",
    "                                name=\"get_metric_domain_subdomain\",\n",
    "                                content=str(result)\n",
    "                            ))\n",
    "                        else:\n",
    "                            result = await self.tools[\"get_metric_domain\"].ainvoke({\n",
    "                                \"model\": model,\n",
    "                                \"domain\": relevant_domain\n",
    "                            })\n",
    "                            result = parse_tool_result(result)\n",
    "                            print(f\"Metrics for {model}/{relevant_domain}: {result}\")\n",
    "                            metrics[f\"{model}_{relevant_domain}\"] = result\n",
    "                            messages.append(ToolMessage(\n",
    "                                tool_call_id=f\"metrics_{model}_{relevant_domain}\",\n",
    "                                name=\"get_metric_domain\",\n",
    "                                content=str(result)\n",
    "                            ))\n",
    "            else:\n",
    "                print(\"Decision: Unknown intent, no metrics collected\")\n",
    "                metrics[\"unknown\"] = {}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in collect_metrics: {e}\")\n",
    "            messages.append(ToolMessage(\n",
    "                tool_call_id=\"metrics_error\",\n",
    "                name=\"metrics\",\n",
    "                content=f\"Error: {e}\"\n",
    "            ))\n",
    "\n",
    "        print(f\"Collected metrics: {metrics}\")\n",
    "        return {\n",
    "            \"step\": \"metrics\",\n",
    "            \"metrics\": metrics,\n",
    "            \"messages\": messages,\n",
    "            \"selected_llm\": selected_llm,\n",
    "            \"context\": state[\"context\"] + f\"\\nSelected LLM: {selected_llm} ({selection_reason})\" if selected_llm else state[\"context\"]\n",
    "        }\n",
    "\n",
    "    async def respond_step(self, state: AgentState):\n",
    "        query = state[\"query\"].lower()\n",
    "        intent = state[\"intent\"]\n",
    "        metrics = state[\"metrics\"]\n",
    "        context = state[\"context\"]\n",
    "        selected_llm = state[\"selected_llm\"]\n",
    "        print(f\"Responding with metrics: {metrics}, Intent: {intent}, Context: {context}, Selected LLM: {selected_llm}\")\n",
    "\n",
    "        response = f\"Based on the analysis and context:\\nContext: {context}\\n\\n\"\n",
    "        if intent == \"list_models\" and \"models\" in metrics:\n",
    "            if metrics[\"models\"]:\n",
    "                response += f\"Available models: {', '.join(metrics['models'])}\"\n",
    "            else:\n",
    "                response += \"No models found in the CSV file.\"\n",
    "        elif intent in [\"compare\", \"metrics\"] and not metrics:\n",
    "            response += \"No specific data found for this query in the CSV. Try a related domain/subdomain or check the query.\"\n",
    "        elif intent == \"compare\" or (intent == \"metrics\" and (\"better\" in query or \"prefer\" in query or \"best\" in query)):\n",
    "            comparison = metrics.get(\"comparison\", {})\n",
    "            if isinstance(comparison, dict) and comparison:\n",
    "                best_model = max(comparison.items(), key=lambda x: x[1], default=(None, 0))[0]\n",
    "                if best_model:\n",
    "                    response += f\"The best model for the query is {best_model} with an average accuracy of {comparison[best_model]:.2f}.\\n\"\n",
    "                    response += \"Model accuracies:\\n\" + \"\\n\".join(f\"{model}: {acc:.2f}\" for model, acc in comparison.items()) + \"\\n\"\n",
    "                    if selected_llm:\n",
    "                        response += f\"Selected LLM: {selected_llm} (chosen for highest accuracy in {query}).\\n\"\n",
    "                else:\n",
    "                    response += f\"No valid comparison data found. Received: {comparison}\\n\"\n",
    "            # Include metrics if requested\n",
    "            if \"metrics\" in query:\n",
    "                response += \"Detailed metrics:\\n\"\n",
    "                for key, metric in metrics.items():\n",
    "                    if \"metrics\" in key and isinstance(metric, dict) and metric:\n",
    "                        response += f\"{key}:\\n\" + \"\\n\".join(f\"{k}: {v:.2f}\" for k, v in metric.items() if isinstance(v, (int, float))) + \"\\n\"\n",
    "                        if \"accuracy\" in metric and metric[\"accuracy\"] < 0.1:\n",
    "                            response += f\"Warning: Low accuracy ({metric['accuracy']:.2f}) for {key}. Verify CSV data.\\n\"\n",
    "                    elif \"metrics\" in key:\n",
    "                        response += f\"No metrics data found for {key}. Received: {metric}\\n\"\n",
    "            # Generate chart for comparison\n",
    "            \n",
    "        elif intent == \"metrics\":\n",
    "            response += \"Metrics for the requested models:\\n\"\n",
    "            accuracies = {}\n",
    "            for key, metric in metrics.items():\n",
    "                if isinstance(metric, dict) and metric:\n",
    "                    response += f\"{key}:\\n\" + \"\\n\".join(f\"{k}: {v:.2f}\" for k, v in metric.items() if isinstance(v, (int, float))) + \"\\n\"\n",
    "                    if \"accuracy\" in metric:\n",
    "                        model_name = key.split(\"_\")[0]\n",
    "                        accuracies[model_name] = metric[\"accuracy\"]\n",
    "                        if metric[\"accuracy\"] < 0.1:\n",
    "                            response += f\"Warning: Low accuracy ({metric['accuracy']:.2f}) for {key}. Verify CSV data.\\n\"\n",
    "                else:\n",
    "                    response += f\"No metrics data found for {key}. Received: {metric}\\n\"\n",
    "            if \"better\" in query or \"prefer\" in query or \"best\" in query:\n",
    "                if accuracies:\n",
    "                    best_model = max(accuracies.items(), key=lambda x: x[1], default=(None, 0))[0]\n",
    "                    if best_model:\n",
    "                        response += f\"The best model is {best_model} with an accuracy of {accuracies[best_model]:.2f}.\\n\"\n",
    "                        if selected_llm:\n",
    "                            response += f\"Selected LLM: {selected_llm} (chosen for highest accuracy in {query}).\\n\"\n",
    "                        \n",
    "                else:\n",
    "                    response += \"No valid accuracy data found for comparison.\\n\"\n",
    "        else:\n",
    "            response += \"No specific data found for this query in the CSV. Try a related domain/subdomain or check the query.\"\n",
    "\n",
    "        return {\n",
    "            \"step\": \"done\",\n",
    "            \"messages\": [AIMessage(content=response)]\n",
    "        }\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3809adb8-e6f5-40cc-838d-a9abcf409f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with query: How do metrics compare for Combinatorics across both models and which one is more efficient in memory usage?\n",
      "Classified intent: list_models\n",
      "Models retrieved: ['Llama3.2_1B', 'Qwen2.5_1.5B']\n",
      "Context: \n",
      "Routing: Intent=list_models, Models=['Llama3.2_1B', 'Qwen2.5_1.5B'], Domains=[], Subdomains={}\n",
      "Routing: Using cached models, skipping list_models\n",
      "Responding with metrics: {}, Intent: list_models, Context: , Selected LLM: None\n",
      "User query: How do metrics compare for Combinatorics across both models and which one is more efficient in memory usage?\n",
      "Agent response:\n",
      "Based on the analysis and context:\n",
      "Context: \n",
      "\n",
      "No specific data found for this query in the CSV. Try a related domain/subdomain or check the query.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # 1) Load your CSV tools via MCP\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    # 2) Initialize your LLM wrapper\n",
    "    model = ChatTogether(\n",
    "        model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 3) Build the Agent with memory checkpointer\n",
    "    agent = Agent(model, tools, checkpointer=memory)\n",
    "\n",
    "    # 4) Prepare test queries\n",
    "    queries = [\n",
    "        \"How do metrics compare for Combinatorics across both models and which one is more efficient in memory usage?\"\n",
    "    ]\n",
    "\n",
    "    # 5) Initialize shared state\n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"step\": \"init\",\n",
    "        \"models\": [],\n",
    "        \"domains\": [],\n",
    "        \"subdomains\": {},\n",
    "        \"query\": \"\",\n",
    "        \"metrics\": {},\n",
    "        \"intent\": \"\",\n",
    "        \"context\": \"\",\n",
    "        \"selected_llm\": None\n",
    "    }\n",
    "\n",
    "    # 6) Iterate through queries\n",
    "    for query in queries:\n",
    "        state[\"query\"] = query\n",
    "        state[\"messages\"] = [HumanMessage(content=query)]\n",
    "\n",
    "        # Use the same thread config if required by your MCP setup\n",
    "        thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "        # Invoke your agent's state graph\n",
    "        result = await agent.graph.ainvoke(state, thread)\n",
    "\n",
    "        # Print the final LLM response\n",
    "        print(f\"User query: {query}\")\n",
    "        print(f\"Agent response:\\n{result['messages'][-1].content}\\n\")\n",
    "\n",
    "        # Update cached fields for potential multi-turn\n",
    "        for field in [\"models\", \"domains\", \"subdomains\", \"context\", \"selected_llm\"]:\n",
    "            if field in result:\n",
    "                state[field] = result[field]\n",
    "\n",
    "        # Append tool & assistant messages to history\n",
    "        state[\"messages\"].extend(result.get(\"messages\", []))\n",
    "\n",
    "# Run the async main (in Jupyter or asyncio)\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
